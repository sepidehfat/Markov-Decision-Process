{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.text_cell .rendered_html * {direction: ltr; text-align: left;}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>.text_cell .rendered_html * {direction: ltr; text-align: left;}</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBMoPLmGbrIn"
   },
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasons can be defined as states, and each episode is contained of 4 states from starting point to the end or goal state.  \n",
    "The policy is the amount of seed that we want to implant.  \n",
    "The situation of weather and dust must be involved in the rewards.  \n",
    "Rewards also depend on the amount of product we have from previous years.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "states: number of guests who register every day.  \n",
    "actions: the number of guests the manager wants to register to the hotel.  \n",
    "rewards: negative comments and the number of rejected quests will appear as punishments and will affect the rewards.  \n",
    "We can start from random policy and improve it in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "states: each device can be defined as one state.  \n",
    "actions: repair, analyze, change,    \n",
    "rewards: if the device is completely out of service and needs, then it is a punishment that will appear in the reward function, and it can get -inf as a reward. For repairing and changing the product, we have punishment as well.   \n",
    "Because each device is different, we should calculate the expected values separately. The discount factor that we use in q-value calculation should be used for each device independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [],
   "source": [
    "from amalearn.environment import EnvironmentBase\n",
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 actions that we can choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = np.array([(0,-1), (-1,0), (0,1), (1,0), (-1,-1), (1,1), (0,0), (-1,1), (1,-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, map_size=17, obstacle=[],goal= np.array([(1,1)]) ,id=0, action_count=9, actionPrice=-1, goalReward=100, punish=-10, j_limit=10, i_limit=10, p=0.8):\n",
    "        \"\"\"\n",
    "        initialize your variables\n",
    "        \"\"\"\n",
    "        self.state = None\n",
    "        self.map_size = map_size\n",
    "        self.action_count = action_count\n",
    "        self.action_price = actionPrice\n",
    "        self.goal_reward = goalReward\n",
    "        self.punishment = punish\n",
    "        self.j_limit = j_limit \n",
    "        self.i_limit = i_limit \n",
    "        self.p = p\n",
    "        self.obstacle = obstacle\n",
    "        self.actions = np.array([(0,-1), (-1,0), (0,1), (1,0), (-1,-1), (1,1), (0,0), (-1,1), (1,-1)])\n",
    "        self.goal = goal\n",
    "        \n",
    "    def isStatePossible(self, state):\n",
    "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
    "        valid = False\n",
    "        if (self.map_size*state[0] + state[1]) not in self.obstacle:\n",
    "            valid = True\n",
    "        if state[0]<0 or state[1]<0 or state[0]>16 or state[1]>16:\n",
    "            valid = False\n",
    "        return valid\n",
    "    \n",
    "    def isAccessible(self, state, state_p):\n",
    "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
    "        if not self.isStatePossible(state_p):\n",
    "            return False\n",
    "        next_states = self.available_states(state)\n",
    "        if state_p in next_states:\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    def getTransitionStatesAndProbs(self, state, action):\n",
    "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
    "        transition_probs= []\n",
    "        transition_states = self.available_states(state)\n",
    "        next_state = action + state\n",
    "        for s in transition_states:\n",
    "            if next_state[0] == s[0] and next_state[1] == s[1]:\n",
    "                transition_probs.append(self.p)\n",
    "            else:\n",
    "                transition_probs.append((1-self.p)/(len(transition_states)-1+0.0000001)) \n",
    "        return transition_states,transition_probs\n",
    "        \n",
    "    \n",
    "    def getReward(self, state, action):\n",
    "        \"\"\"return reward of transition\"\"\"\n",
    "        rewards = []\n",
    "        available_states = self.available_states(state)\n",
    "        next_state = state + action\n",
    "        for s in available_states:\n",
    "            r = 0\n",
    "            if not self.isStatePossible(next_state):\n",
    "                r = -1\n",
    "            if next_state[0] != s[0] and next_state[1] != s[1]:\n",
    "                r -= 0.01\n",
    "            if self.goal[0] == s[0] and self.goal[1] == s[1]:\n",
    "                r += 5\n",
    "            rewards.append(r)\n",
    "        return rewards\n",
    "        \n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        return \n",
    "\n",
    "    def terminated(self):\n",
    "        return \n",
    "\n",
    "    def observe(self):\n",
    "        return \n",
    "\n",
    "    def available_actions(self, current_state):\n",
    "        available_actions = []\n",
    "        for a in self.actions:\n",
    "            next_state = current_state + a\n",
    "            if self.isStatePossible(next_state):\n",
    "                available_actions.append(a)\n",
    "        return available_actions\n",
    "    \n",
    "    def available_states(self, current_state):\n",
    "        next_states = []\n",
    "        available_actions = self.available_actions(current_state)\n",
    "        for a in available_actions:            \n",
    "            next_states.append(a+current_state)\n",
    "        return next_states\n",
    "            \n",
    "    def next_state(self, action):\n",
    "        return self.state + action\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([(map_size, map_size)])\n",
    "        print(\"current state: \", self.state)\n",
    "        return \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        #print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
    "        return \n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id, environment, discount, theta):\n",
    "        self.environment = environment\n",
    "        self.V = np.zeros((17, 17))\n",
    "        self.policy = np.zeros((17, 17))\n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "    \n",
    "    def sum_Q_value(self, s0, s1, a):\n",
    "        sum_q_value = 0\n",
    "        transition_states,transition_probs = self.environment.getTransitionStatesAndProbs((s0,s1),a)\n",
    "        rewards = self.environment.getReward((s0,s1),a)\n",
    "        for i,s in enumerate(transition_states):\n",
    "            sum_q_value +=  transition_probs[i]*(rewards[i] + self.discount*(self.V[s[0]][s[1]]))\n",
    "        return sum_q_value\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s0 in range(17):\n",
    "                for s1 in range(17):\n",
    "                    if not self.environment.isStatePossible((s0,s1)):\n",
    "                        continue\n",
    "                    a = self.policy[s0][s1]\n",
    "                    v = self.V[s0][s1]\n",
    "                    self.V[s0][s1] = self.sum_Q_value(s0, s1, ACTIONS[int(a)])\n",
    "                    delta = max(delta, abs(v-self.V[s0][s1]))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "            \n",
    "    def policy_improvement(self):\n",
    "        print(\"policy imp\")\n",
    "        \n",
    "        policy_stable = True\n",
    "        for s0 in range(17):\n",
    "            for s1 in range(17):\n",
    "                if not self.environment.isStatePossible((s0,s1)):\n",
    "                    continue\n",
    "                max_q = 0\n",
    "                for index_a,a in enumerate(ACTIONS):\n",
    "                    transition_states,transition_probs = self.environment.getTransitionStatesAndProbs((s0,s1),a)\n",
    "                    rewards = self.environment.getReward((s0,s1),a)\n",
    "                    q_value = 0\n",
    "                    for i,s in enumerate(transition_states):\n",
    "                        q_value +=  transition_probs[i]*(rewards[i] + self.discount*(self.V[s[0]][s[1]]))\n",
    "                    if q_value > max_q:\n",
    "                        max_q = q_value\n",
    "                        max_a = index_a\n",
    "                self.policy[s0][s1] = max_a\n",
    "                if self.policy[s0][s1] != max_a:\n",
    "                    policy_stable = False\n",
    "        print(self.policy)\n",
    "        return policy_stable\n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        policy_stable = False\n",
    "        iter=0\n",
    "        while not policy_stable:\n",
    "            self.policy_evaluation()\n",
    "            policy_stable = self.policy_improvement()\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obstacles(map):\n",
    "    n = len(map)\n",
    "    obstacles= []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if map[i][j] == \"1\":\n",
    "                obstacles.append(n*i+j)\n",
    "    return obstacles\n",
    "\n",
    "def get_goal(map):\n",
    "    n = len(map)\n",
    "    goals= []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if map[i][j] == \"G\":\n",
    "                goals.append(np.array([i,j]))\n",
    "    return goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 17\n",
    "map =  [\n",
    "        \"11111111111111111\",\n",
    "        \"1G000001100000001\",\n",
    "        \"10000001100000001\",\n",
    "        \"10000001100000001\",\n",
    "        \"10000001100000001\",\n",
    "        \"10000000000000001\",\n",
    "        \"10000000000000001\",\n",
    "        \"10000000000000001\",\n",
    "        \"10000000000001111\",\n",
    "        \"10000000000001111\",\n",
    "        \"10000000000000001\",\n",
    "        \"10000000000000001\",\n",
    "        \"10000011000000001\",\n",
    "        \"10000011000000001\",\n",
    "        \"10000011000000001\",\n",
    "        \"100000110000000S1\",\n",
    "        \"11111111111111111\"]\n",
    "\n",
    "obstacles = get_obstacles(map)\n",
    "goal = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available\n"
     ]
    }
   ],
   "source": [
    "state = (1,1)\n",
    "if (17*state[0] + state[1]) not in obstacles:\n",
    "    print(\"available\")\n",
    "else:\n",
    "    print(\"not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "state1 = np.array([1,1])\n",
    "state2 = np.array([1,1])\n",
    "if state1 is state2:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state3 = np.array([(0,0), (1,1)])\n",
    "state3[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(17, obstacles,goal=np.array([1,1]),id = \"0\", action_count=9, actionPrice=-0.01, goalReward=100, punish=-1, j_limit=10, i_limit=10, p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent('1', env, discount=0.9, theta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy imp\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 4. 4. 4. 4. 4. 0. 0. 4. 4. 4. 4. 4. 4. 4. 0.]\n",
      " [0. 7. 7. 7. 7. 1. 4. 0. 0. 4. 4. 4. 4. 4. 4. 4. 0.]\n",
      " [0. 4. 4. 4. 4. 7. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "agent.policy_iteration()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
